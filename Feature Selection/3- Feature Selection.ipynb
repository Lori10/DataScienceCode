{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection General approach\n",
    "\n",
    "A)\n",
    "1. Choose a classifier that you think might work well in that particular case and very loosely optimize hyperparameters, just to make sure you don't assign extremely bad values to some hyperparameters. This can often just be done by hand if you have a good intuitive understanding of your hyperparameters, or done with a very brief hyperparameter optimization procedure using just a bunch of features that you know to be decently good otherwise.\n",
    "\n",
    "2. Feature selection, with hyperparameters that are maybe not 100% optimized but at least not extremely terrible either. If you have at least a somewhat decently configured machine learning algorithm already, having good features will be significantly more important for your performance than micro-optimizing hyperparameters. Extreme examples: If you have no features, you can't predict anything. If you have a cheating feature that contains the class label, you can perfectly classify everything.\n",
    "\n",
    "3. Optimize hyperparameters with the features selected in the step above. This should be a good feature set now, where it actually may be worth optimizing hyperparams a bit.\n",
    "\n",
    "The disadvantage of this approach is that we might have data leakage.\n",
    "\n",
    "B) If we have time to optimize our model we can include feature selection in the cross validation process. This would be a better aproach as it would avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques\n",
    "1. Recursive Elemination with Cross Validation\n",
    "2. Univariate Selection (SelectKBest commonly used)\n",
    "3. SelectFromModel\n",
    "4. Sequential Features Selection (Forward or Backward)\n",
    "5. Feature Selection Tuning with Hyperparameters using Pipelines (best approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('mobile_dataset.csv')\n",
    "X=df.iloc[:,:-1]\n",
    "y=df['price_range']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train.shape # 20 total features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recursive Elemination with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the feature_importance of the estimator that we specify it will rank the features from the most important to the least important. In each step it will decrease the nr of features by a specified value and perform cross validation. If the score of Cross Validation decreases it will stop decreasing the nr of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['battery_power', 'px_height', 'px_width', 'ram', 'pc', 'talk_time', 'sc_h', 'mobile_wt', 'int_memory', 'sc_w', 'n_cores', 'fc', 'm_dep', 'touch_screen', 'wifi', 'clock_speed', 'four_g', 'three_g', 'blue', 'dual_sim']\n"
     ]
    }
   ],
   "source": [
    "dic = {'battery_power': 1, 'px_height': 1, 'px_width': 1, 'ram': 1, 'pc': 2, 'talk_time': 3, 'sc_h': 4, 'mobile_wt': 5, 'int_memory': 6, 'sc_w': 7, 'n_cores': 8, 'fc': 9, 'm_dep': 10, 'touch_screen': 11, 'wifi': 12, 'clock_speed': 13, 'four_g': 14, 'three_g': 15, 'blue': 16, 'dual_sim': 17}\n",
    "print(list(dic.keys()))\n",
    "#plt.barh(width=ranking , y=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feature_selection_recursive_elemination_cv(model, X_train, y_train, X_test, scoring, step=1, min_features_to_select=1, \n",
    "                                               cv_kfold=5, verbose=False):\n",
    "    try:\n",
    "        print('Entered function feature_selection_recursive_elemination_cv.')\n",
    "\n",
    "        total_nr_features = X_train.shape[1]\n",
    "        selector = RFECV(model, step=step, min_features_to_select=min_features_to_select, cv=cv_kfold, scoring=scoring, \n",
    "                         n_jobs=-1, verbose=verbose)\n",
    "        selector.fit(X_train, y_train)\n",
    "        nr_features_selected = selector.n_features_\n",
    "        \n",
    "        dicti = {}\n",
    "        for col, ranking in zip(X_train.columns, selector.ranking_):\n",
    "            dicti[col] = ranking\n",
    "        \n",
    "        \n",
    "        sorted_keys = sorted(dicti, key=dicti.get)\n",
    "        sorted_dicti = {key: dicti[key] for key in sorted_keys}\n",
    "        print('Features Ranking starting from the most important feature : ' + str(sorted_dicti))\n",
    "        \n",
    "        ranking = sorted_dicti.values()\n",
    "        feature_names = list(sorted_dicti.keys())\n",
    "        \n",
    "        plt.barh(width=ranking, y=feature_names)\n",
    "        plt.title(\"Feature Ranking (Rank 1 means Best Feature)\")\n",
    "        plt.xlabel('Ranking (Rank 1 means best feature)')\n",
    "        plt.ylabel('Feature Names')\n",
    "        plt.savefig('Feature_Importance.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        cols_selected = X_train.columns[selector.support_]\n",
    "        cols_removed = [col for col in X_train.columns if col not in cols_selected]\n",
    "        print('Features Selected : ')\n",
    "        print(cols_selected)\n",
    "        print('Features Removed : ')\n",
    "        print(cols_removed)\n",
    "        \n",
    "        new_X_train = X_train[cols_selected]\n",
    "        new_X_test = X_test[cols_selected]\n",
    "        \n",
    "        \n",
    "        print('Function feature_selection_recursive_elemination_cv Completed Successfully. Exited this function.')\n",
    "        return new_X_train, new_X_test\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in function feature_selection_recursive_elemination_cv. Error Message : ' + str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered function feature_selection_recursive_elemination_cv.\n",
      "Features Ranking starting from the most important feature : {'battery_power': 1, 'px_height': 1, 'px_width': 1, 'ram': 1, 'pc': 2, 'talk_time': 3, 'sc_h': 4, 'mobile_wt': 5, 'int_memory': 6, 'sc_w': 7, 'n_cores': 8, 'fc': 9, 'm_dep': 10, 'touch_screen': 11, 'wifi': 12, 'clock_speed': 13, 'four_g': 14, 'three_g': 15, 'blue': 16, 'dual_sim': 17}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Selected : \n",
      "Index(['battery_power', 'px_height', 'px_width', 'ram'], dtype='object')\n",
      "Features Removed : \n",
      "['blue', 'clock_speed', 'dual_sim', 'fc', 'four_g', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'touch_screen', 'wifi']\n",
      "Function feature_selection_recursive_elemination_cv Completed Successfully. Exited this function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1315</td>\n",
       "      <td>59</td>\n",
       "      <td>575</td>\n",
       "      <td>3278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>989</td>\n",
       "      <td>46</td>\n",
       "      <td>1069</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1229</td>\n",
       "      <td>120</td>\n",
       "      <td>1878</td>\n",
       "      <td>1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>920</td>\n",
       "      <td>1058</td>\n",
       "      <td>1421</td>\n",
       "      <td>2608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1654</td>\n",
       "      <td>1184</td>\n",
       "      <td>1719</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  px_height  px_width   ram\n",
       "650            1315         59       575  3278\n",
       "1104            989         46      1069  1737\n",
       "561            1229        120      1878  1667\n",
       "643             920       1058      1421  2608\n",
       "441            1654       1184      1719  1070"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "new_X_train, new_X_test = feature_selection_recursive_elemination_cv(model=XGBRegressor(random_state=1), X_train=X_train, \n",
    "                                                                     y_train=y_train, X_test=X_test, \n",
    "                                                                     scoring='neg_mean_squared_error')\n",
    "\n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Univariate Selection (SelectKBest commonly used)\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. It will measure how strong is the relationship between independent and target feature and rank the features based on their score.\n",
    "\n",
    "A) SelectKBest removes all but the  highest scoring features (select the top k most important features)\n",
    "\n",
    "B) SelectPercentile removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.\n",
    "\n",
    "C) GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):\n",
    "\n",
    "Statistical test For regression: f_regression, mutual_info_regression\n",
    "\n",
    "Statistical test For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example i will use SelectKBest which is more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def feature_selection_univariate_selection(X_train, y_train, X_test, nr_features_to_select, score_func=chi2):\n",
    "    try:\n",
    "        print('Entered function feature_selection_univariate_selection.')\n",
    "        total_nr_features = X_train.shape[1]\n",
    "        print(total_nr_features)\n",
    "        selector = SelectKBest(score_func=score_func, k=nr_features_to_select)\n",
    "        ordered_feature = selector.fit(X_train, y_train)\n",
    "    \n",
    "        indices_of_selected_features = ordered_feature.get_support(indices=True)\n",
    "        cols_selected = X_train.columns[indices_of_selected_features]\n",
    "        nr_features_selected = len(cols_selected)\n",
    "        print(cols_selected)\n",
    "        \n",
    "    \n",
    "        new_X_train = pd.DataFrame(data=selector.transform(X_train), columns=cols_selected)\n",
    "        new_X_test = pd.DataFrame(data=selector.transform(X_test), columns=cols_selected)\n",
    "    \n",
    "        dfscores=pd.DataFrame(ordered_feature.scores_,columns=[\"Score\"])\n",
    "        dfcolumns=pd.DataFrame(X_train.columns)\n",
    "        features_rank=pd.concat([dfcolumns,dfscores],axis=1)\n",
    "        features_rank.columns=['Features','Score']\n",
    "        new_features_rank = features_rank.sort_values(by=['Score'], axis=0, ascending=False)\n",
    "        new_features_rank.to_csv('Features_Ranking.csv', index=False)\n",
    "  \n",
    "        print('Function feature_selection_univariate_selection Completed Successfully. Exited this function.')\n",
    "        return new_X_train, new_X_test \n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in function feature_selection_univariate_selection. Error Message : ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered function feature_selection_univariate_selection.\n",
      "20\n",
      "Index(['battery_power', 'mobile_wt', 'px_height', 'px_width', 'ram'], dtype='object')\n",
      "Function feature_selection_univariate_selection Completed Successfully. Exited this function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1315.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>3278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>989.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1229.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>920.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>2608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>1070.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  mobile_wt  px_height  px_width     ram\n",
       "0         1315.0      128.0       59.0     575.0  3278.0\n",
       "1          989.0      124.0       46.0    1069.0  1737.0\n",
       "2         1229.0      191.0      120.0    1878.0  1667.0\n",
       "3          920.0      149.0     1058.0    1421.0  2608.0\n",
       "4         1654.0      146.0     1184.0    1719.0  1070.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testin the function\n",
    "\n",
    "new_X_train, new_X_test = feature_selection_univariate_selection(X_train, y_train, X_test, nr_features_to_select=5, score_func=chi2)\n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SelectFromModel\n",
    "SelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as coef_, feature_importances_) or via an importance_getter callable after fitting. If the feature_importance is lower than the feature_importance threshold we specify, it is discarded, otherwise it will be selected.  <br>\n",
    "It can be used with any ML Estimators that calculate coef_ like Lasso, or feature_importances_ like ExtraTreeClassifier, XGBoost etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform Regularization using 3 different ML Estimators. <br>\n",
    "A) Regularization with Linear Regression we use Lasso or ElasticNet. <br>\n",
    "B) Regularization with Logistic Regression we use LogisticRegression(penalty='l1'). <br>\n",
    "C) Regularization with Support Vector Machine we use LinearSCV(penalty='l1'). <br>\n",
    "\n",
    "penalty : possible values 'l1', 'l2', 'elasticnet'. Usually for feature selection 'l1' and elastic net are used. <br>\n",
    "solver : the algorithm to use in the optimization : possible values : ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’. Check sklearn documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def feature_selection_SelectFromModel(X_train, y_train, X_test, model, threshold=None):\n",
    "    try:\n",
    "        print('Entered function feature_selection_SelectFromModel.')\n",
    "        \n",
    "        nr_total_features = X_train.shape[1]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        selector = SelectFromModel(model, prefit=True, threshold=threshold)\n",
    "        indices_selected_cols = list(selector.get_support(indices=True))\n",
    "        cols_selected = X_train.columns[indices_selected_cols]\n",
    "        nr_features_selected = len(cols_selected)\n",
    "        print(cols_selected)\n",
    "\n",
    "        new_X_train = pd.DataFrame(data=selector.transform(X_train), columns=cols_selected)\n",
    "        new_X_test = pd.DataFrame(data=selector.transform(X_test), columns=cols_selected)\n",
    "        \n",
    "        # If estimator measures the feature importance feature_importance use this code to plot the feature importances \n",
    "        # if it uses other measurement like coef_ or others using this code will not work\n",
    "        \n",
    "        #ranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\n",
    "        #ranked_features.nlargest(10).plot(kind='barh')\n",
    "        #plt.savefig('Feature_Ranking.jpg')\n",
    "        #plt.show()\n",
    "        \n",
    "        print('Function feature_selection_SelectFromModel Completed Successfully. Exited this function')\n",
    "        return new_X_train, new_X_test\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in function feature_selection_SelectFromModel. Error Message : ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 20)\n",
      "Entered function feature_selection_SelectFromModel.\n",
      "Index(['battery_power', 'ram'], dtype='object')\n",
      "Function feature_selection_SelectFromModel Completed Successfully. Exited this function\n",
      "(1500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Testing the function using ExtraTreesClassifier\n",
    "# threshold will have default value\n",
    "\n",
    "print(X_train.shape)\n",
    "new_X_train, new_X_test = feature_selection_SelectFromModel(X_train, y_train, X_test, ExtraTreesClassifier())\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 20)\n",
      "Entered function feature_selection_SelectFromModel.\n",
      "Index(['battery_power', 'clock_speed', 'dual_sim', 'fc', 'int_memory',\n",
      "       'mobile_wt', 'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h',\n",
      "       'sc_w', 'talk_time'],\n",
      "      dtype='object')\n",
      "Function feature_selection_SelectFromModel Completed Successfully. Exited this function\n",
      "(1500, 14)\n"
     ]
    }
   ],
   "source": [
    "# Testing the function using LinearSCV with L1 Regularization estimator\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "print(X_train.shape)\n",
    "estimator = LinearSVC(C=0.01, penalty='l1', dual=False)\n",
    "new_X_train, new_X_test = feature_selection_SelectFromModel(X_train, y_train, X_test, estimator)\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 20)\n",
      "Entered function feature_selection_SelectFromModel.\n",
      "Index(['battery_power', 'int_memory', 'mobile_wt', 'n_cores', 'pc',\n",
      "       'px_height', 'px_width', 'ram', 'sc_h', 'talk_time'],\n",
      "      dtype='object')\n",
      "Function feature_selection_SelectFromModel Completed Successfully. Exited this function\n",
      "(1500, 10)\n"
     ]
    }
   ],
   "source": [
    "# Testing the function using LogisticRegression with L1 Regularization estimator\n",
    "# in case of regression we can use Linear Regression we use Lasso or ElasticNet\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(X_train.shape)\n",
    "estimator=LogisticRegression(penalty='l1', C=0.01, solver='liblinear')\n",
    "new_X_train, new_X_test = feature_selection_SelectFromModel(X_train, y_train, X_test, estimator)\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sequential Feature Selector (SFS)\n",
    "\n",
    "Sequential Feature Selection [sfs] (SFS) is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n",
    "\n",
    "Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature (best feature) that maximizes a cross-validated score when an estimator is trained on this single feature (we train an ML Estimator using each single independent feature and select the one with highest cross validation score). Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\n",
    "\n",
    "Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "def feature_selection_SFS(X_train, y_train, X_test, model, direction='forward', n_features_to_select=None,\n",
    "                                      scoring=None, cv=None):\n",
    "    try:\n",
    "        print('Entered function feature_selection_SFS.')\n",
    "        \n",
    "        nr_total_features = X_train.shape[1]\n",
    "\n",
    "        selector = SequentialFeatureSelector(estimator=model, n_features_to_select=n_features_to_select, direction=direction, \n",
    "                                             scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        \n",
    "        selector.fit(X_train, y_train)\n",
    "        \n",
    "        indices_selected_cols = list(selector.get_support(indices=True))\n",
    "        cols_selected = X_train.columns[indices_selected_cols]\n",
    "        nr_features_selected = len(cols_selected)\n",
    "        print(cols_selected)\n",
    "\n",
    "        new_X_train = pd.DataFrame(data=selector.transform(X_train), columns=cols_selected)\n",
    "        new_X_test = pd.DataFrame(data=selector.transform(X_test), columns=cols_selected)\n",
    "        \n",
    "        # If estimator has feature_importance use this code to plot the feature importances \n",
    "        \n",
    "        #ranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\n",
    "        #ranked_features.nlargest(10).plot(kind='barh')\n",
    "        #plt.savefig('Feature_Ranking.jpg')\n",
    "        #plt.show()\n",
    "        \n",
    "        print('Function feature_selection_SFS Completed Successfully. Exited this function')\n",
    "        return new_X_train, new_X_test\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in function feature_selection_SFS. Error Message : ' + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 20)\n",
      "Entered function feature_selection_SFS.\n",
      "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
      "       'int_memory'],\n",
      "      dtype='object')\n",
      "Function feature_selection_SFS Completed Successfully. Exited this function\n",
      "(1500, 7)\n"
     ]
    }
   ],
   "source": [
    "# Testing function using forward SFS\n",
    "print(X_train.shape)\n",
    "new_X_train, new_X_test = feature_selection_SFS(X_train, y_train, X_test, LogisticRegression(), direction='forward',\n",
    "                                                n_features_to_select=7, scoring='roc_auc')\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 20)\n",
      "Entered function feature_selection_SFS.\n",
      "Index(['ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'touch_screen', 'wifi'], dtype='object')\n",
      "Function feature_selection_SFS Completed Successfully. Exited this function\n",
      "(1500, 7)\n"
     ]
    }
   ],
   "source": [
    "# Testing function using backward SFS\n",
    "print(X_train.shape)\n",
    "new_X_train, new_X_test = feature_selection_SFS(X_train, y_train, X_test, LogisticRegression(), direction='backward',\n",
    "                                                n_features_to_select=7, scoring='roc_auc')\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Selection Tuning with Hyperparameters using Pipelines (best approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuning Feature Selection and Hyperparameters at the same time. We will perform CV on all possible combinations of nr of features and hyperparameters and find best model.\n",
    "* Advantage of this approach is that we are going to find the model with highest performance but the disadvantage is that it requires too much time for the tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like you already observed yourself, your choice of features (feature selection) may have an impact on which hyperparameters for your algorithm are optimal, and which hyperparameters you select for your algorithm may have an impact on which choice of features would be optimal.\n",
    "\n",
    "So, yes, if you really really care about squeezing every single percent of performance out of your model, and you can afford the required amount of computation, the best solution is probably to do feature selection and hyperparamter tuning \"at the same time\". That's probably not easy (depending on how you do feature selection) though. The way I imagine it working would be like having different sets of features as candidates, and treating the selection of one set of features out of all those candidate sets as an additional hyperparameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y= make_regression(n_samples=2000, n_features=20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def feature_selection_hyperparameter_tuning(X_train, y_train, pipeline, params, scoring, cv=5, n_iter=20, verbose=False):\n",
    "    try:\n",
    "        print('Entered function feature_selection_hyperparameter_tuning.')\n",
    "        \n",
    "        infos = {}\n",
    "    \n",
    "        search = RandomizedSearchCV(pipeline, params, scoring=scoring, n_jobs=-1, n_iter=n_iter, cv=cv, verbose=verbose,\n",
    "                                random_state=1)\n",
    "        search.fit(X_train, y_train)\n",
    "    \n",
    "        infos['tuned_model'] = search.best_estimator_\n",
    "        infos['best_cv_score'] = search.best_score_\n",
    "        infos['best_hyperparameters_nr_features_selected'] = search.best_params_\n",
    "    \n",
    "        print('Function feature_selection_hyperparameter_tuning Completed Successfylly. Exited this function.')\n",
    "        return infos\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in function feature_selection_hyperparameter_tuning. Error Message : ' + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered function feature_selection_hyperparameter_tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 16 is smaller than n_iter=20. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function feature_selection_hyperparameter_tuning Completed Successfylly. Exited this function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tuned_model': Pipeline(steps=[('feature_selector',\n",
       "                  SelectKBest(k=8,\n",
       "                              score_func=<function f_regression at 0x000001481530C9D0>)),\n",
       "                 ('estimator',\n",
       "                  RandomForestRegressor(max_depth=10, n_estimators=300,\n",
       "                                        random_state=1))]),\n",
       " 'best_cv_score': 0.8219013932421891,\n",
       " 'best_hyperparameters_nr_features_selected': {'feature_selector__k': 8,\n",
       "  'estimator__n_estimators': 300,\n",
       "  'estimator__max_depth': 10}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# k=10 by default; k is the nr features selected\n",
    "pipeline = Pipeline([('feature_selector', SelectKBest(score_func=f_regression, k=10)),\n",
    "                     ('estimator', RandomForestRegressor(random_state=1))])\n",
    "\n",
    "params = {'feature_selector__k' : [6, 8, 9, 11],\n",
    "          'estimator__n_estimators' : [100, 300],\n",
    "          'estimator__max_depth' : [6, 10]}\n",
    "\n",
    "dic = feature_selection_hyperparameter_tuning(X_train, y_train, pipeline, params, scoring='r2')\n",
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "\n",
    "1. The advantage of RECV over SFS, SelectFromModel, SElectKBest is that they automatically select the top k best features for our model while for SFS, SelectFromModel, SelectKBest we have to specify the threshold/nr features we want to select which always seem to be a difficult decision.\n",
    "2. SelectFromModel and SelectKBest is significantly faster than SFS and RECV. Indeed, SelectFromModel and SelectKBest only needs to fit a model once, while SFS and RECV needs to cross-validate many different models for each of the iterations. \n",
    "3. SFS and RECV however works with any model, while SelectFromModel requires the underlying estimator to expose a coef_ attribute or a feature_importances_ attribute. \n",
    "4. Comparison between Forward SFS and Backward SFS : The forward SFS is faster than the backward SFS if the nr of features is low, otherwise it is lower than the backward SFS. Suppose if we have total nr of features 20 and the top features that give best performance are 4. In this case forward SFS is faster because it will perform only 4 time CV while backward SFS would perform it 16 times. If top features that give best performance is 17, backward SFS is faster because it will perform cross validation only 3 times while forward SFS would perform it 17 times. So if we think we have more than 50% of features important we should better go for backward SFS to save some time, otherwise use Forward SFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

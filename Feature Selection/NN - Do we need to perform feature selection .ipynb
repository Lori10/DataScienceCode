{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need to perform feature selection in Deep Neural Networks ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature selection can automatically be performed by neural networks in conjunction with L1-regularization. It will assign low weights to unimportant features and higher weights to important features. So in generall there is no need to perform feature selection when using DNN\n",
    "\n",
    "* But if we care about squeezing every single model performance we could go for feature selection. Some reasons : <br>\n",
    "1. The feature is noisy: suppose we are trying to predict whether an object is a cat and we have a feature that tells us how many legs the object has. Intutively, we would expect such a feature to be highly useful. But what if our data collection process was super-noisy for that feature? The result is that we’ll have a whole bunch of bad examples where the data might say a car has 2 legs or a laptop has 3 (and of course they should all have 0 legs). One of two things then might happen. Either the network is able to figure out that this feature is un-reliable and should not be used, or it’ll learn noisy correlations that makes the model perform worse at test time.\n",
    "2. The feature is under-represented in the data (multicolleniarity): this point is partly related to the point above. If a feature is rarely active for the inputs it corresponds to, it is highly unlikely that the network will learn anything meaningful from it. Using the example earlier, this might occur in scenarios where we rarely have knowledge for the number of legs in an object (so it might be given a value of -1 to indicate ‘NULL’). Including this feature can lead to overfitting, since the model might become over-reliant on this feature for the few examples where it is actually used.\n",
    "3. The feature doesn’t actually predict anything: this scenario might occur if our data-collection process was too comprehensive and we end up collecting a bunch of features that are irrelevant for the prediction task. Whether or not the network will be able to learn to ignore these features is highly dependent on the size of the training set and how often this feature is active. In the worst case, the network will learn meaningless correlations that ends up degrading performance at test time.\n",
    "* If we decide to go for feature selection one solution is PCA (Dimensionality Reducation Technique) whichs performs feature extraction.\n",
    "* A common theme in all the points mentioned above: that would be the issue of overfitting. And the easiest way to address all these points is to simply collect more data, use any technique to avoid overfitting— especially for examples that are under-represented in the training set. \n",
    "* We want the model to figure out which features are useful and which ones are not. We don’t want to spend a whole bunch of time analyzing the data and trying to hand-engineer the best set of features. The reasons being that (1) hand-engineering is a lot of effort and cuts into dev-time; (2) no matter how clever we are, we could be wrong on our intuitions/analysis about the data; and (3) its boring. So we should really only focus on feature-selection when getting more data is not an option (e.g. in Kaggle competitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are some values missing?**\n",
    "1. They hesitate to put down the information\n",
    "2. Survey informations are not that valid\n",
    "3. Men--salary (there are men who dont want to show their salary)\n",
    "4. Women---age (there are women who dont want to show their age)\n",
    "5. People may have died----NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the different types of Missing Data?**\n",
    "\n",
    "**1. Missing Completely at Random, MCAR:**<br>\n",
    "A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. When data is MCAR, there is absolutely no relationship between the data missing and any other values (like target feature), observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other. <br>\n",
    "\n",
    "**2. Missing Data Not At Random(MNAR): Systematic missing Values.** <br>\n",
    "There is absolutely some relationship between the data missing and any other values, observed or missing, within the dataset. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Missing At Random(MAR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the techniques of handling missing values for numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean/ Median/Mode replacement\n",
    "2. Iterative Imputer (predicting missing values)\n",
    "3. KNN Imputer\n",
    "2. Random Sample Imputation\n",
    "3. Capturing NAN values with a new feature\n",
    "4. End of Distribution imputation\n",
    "5. Arbitrary imputation\n",
    "6. Frequent categories imputation\n",
    "7. Dropping rows with missing values\n",
    "8. Dropping columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean/ Median/Mode replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works ?** <br>\n",
    "We replace the missing value of a particular column with the mean/mode/median of that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should we apply?** <br>\n",
    "Mean/median imputation has the assumption that the data are missing completely at random(MCAR). We solve this by replacing the NAN with the most frequent occurance of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**<br>\n",
    "1. Easy to implement(Robust to outliers)\n",
    "2. Faster way to obtain the complete dataset\n",
    "<br>\n",
    "**Disadvantages**<br>\n",
    "1. Change or Distortion in the original variance\n",
    "2. Impacts Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** <br>\n",
    "If feature has many outliers we replace with median since it is not affected by outliers, otherwise with mean.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Way\n",
    "# DYNAMIC\n",
    "# Instead of mean we can use mode or median\n",
    "\n",
    "def impute_nan(df, features):\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(df[feature].mean()) # or we can use inplace=True\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Age         0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Age'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "strategy can be mean/mode/most_frequent(median) or constant if we want to fill nan values with a particular number using fill_value. <br>\n",
    "missing_values means how are missing values denoted. They can denoted as np.nan or ' ' or ? etc<br>\n",
    "fill_value=x if we want to fill missing values with number x <br>\n",
    "add_indicator=True if we want to add new features that captures the nan values (1 if value is missing, 0 otherwise)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we set add_indicator=True we must avoid convertin array to dataframe because it will generate an error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Way\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def impute_nan_SingleImputer(X_train, X_test, missing_values=np.nan, strategy='mean', fill_value=None, add_indicator=False):\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=missing_values, strategy=strategy, fill_value=fill_value,\n",
    "                            add_indicator=add_indicator)\n",
    "    arr_train = imputer.fit_transform(X_train)\n",
    "    new_X_train =pd.DataFrame(arr_train, columns=X_train.columns)\n",
    "\n",
    "    arr_test = imputer.transform(X_test)\n",
    "    new_X_test = pd.DataFrame(arr_test, columns=X_test.columns)\n",
    "    \n",
    "    return new_X_train, new_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train, new_X_test = impute_nan_SingleImputer(X_train, X_test, missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>29.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>7.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.639409</td>\n",
       "      <td>14.4583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age     Fare\n",
       "0  37.000000   7.9250\n",
       "1   8.000000  29.1250\n",
       "2  33.000000   7.7750\n",
       "3  16.000000   7.7500\n",
       "4  29.639409  14.4583"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.639409</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.639409</td>\n",
       "      <td>14.4583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>14.4583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age      Fare\n",
       "0  23.000000  263.0000\n",
       "1  29.639409    8.0500\n",
       "2  29.639409   14.4583\n",
       "3  17.000000   14.4583\n",
       "4  51.000000    8.0500"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Iterative Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works?**<br>\n",
    "This approach predicts the missing values using an ML Estimator based on other features. ML model treats each feature with missing values as a function of other features (target feature), and uses that estimate for imputation. (training data will be whereever the values of that feature are not missing, test data will be the data where the values are missing). It does so in an iterated round-robin fashion: at each step, a feature column (with missing values) is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "Since it will fit a ML Estimator to the data, data must be beforehand all in numerical format. <br>\n",
    "Using transform for test data means we are using for imputin nan values of  the test data the same estimator that we fitted to the traiing data. <br>\n",
    "We can use different ML Estimators with Iterative Imputer to predict the missing values like DecisionTreeRegressor, ExtraTreeREgressor, KNearestRegressor, BayesianRidge etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def impute_nan(X_train, X_test, max_iter=10, random_state=1):\n",
    "    \n",
    "    imputer = imputer = IterativeImputer(max_iter=max_iter, random_state=random_state)\n",
    "    arr_train = imputer.fit_transform(X_train)\n",
    "    new_X_train =pd.DataFrame(arr_train, columns=X_train.columns)\n",
    "\n",
    "    arr_test = imputer.transform(X_test)\n",
    "    new_X_test = pd.DataFrame(arr_test, columns=X_test.columns)\n",
    "    \n",
    "    return new_X_train, new_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train, new_X_test = impute_nan(X_train, X_test, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>73.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>20.5250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.490884</td>\n",
       "      <td>39.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>110.8833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age      Fare\n",
       "0  29.000000  211.3375\n",
       "1  21.000000   73.5000\n",
       "2  33.000000   20.5250\n",
       "3  29.490884   39.6000\n",
       "4  17.000000  110.8833"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>39.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>134.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.8542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>8.3625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age      Fare\n",
       "0  41.0   39.6875\n",
       "1  40.0  134.5000\n",
       "2  18.0    7.8542\n",
       "3  32.0    8.3625\n",
       "4  43.0    8.0500"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNN Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works?**<br>\n",
    "The KNNImputer uses using the k-Nearest Neighbors algorithm. \n",
    "A) For each record/row that contains missing value it will measure the euclidian distance of that record with all other records. <br>\n",
    "B) It will select the k nearest neighbors (rows/records) which means the k records with lowest euclidian distance. <br>\n",
    "C) It will take the values of columns that had missing value of the k nearest records, average them and assign to the missing value. If we assign weights to the records the average value will be calculated based on the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "The dataset passed to KNN Imputer must be all in numerical format. <br>\n",
    "Using transform to test data means we will use the same neighbors that we used in training data to impute the missing values of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute_nan(X_train, X_test, missing_values=np.nan, n_neighbors=5, weights='uniform', metric='nan_euclidean',\n",
    "               add_indicator=False):\n",
    "    \n",
    "    imputer = KNNImputer(missing_values=missing_values, n_neighbors=n_neighbors, \n",
    "                         weights=weights, metric=metric, add_indicator=add_indicator)\n",
    "    arr_train = imputer.fit_transform(X_train)\n",
    "    new_X_train =pd.DataFrame(arr_train, columns=X_train.columns)\n",
    "\n",
    "    arr_test = imputer.transform(X_test)\n",
    "    new_X_test = pd.DataFrame(arr_test, columns=X_test.columns)\n",
    "    \n",
    "    return new_X_train, new_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train, new_X_test = impute_nan(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>13.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.2</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>15.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>7.8542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age     Fare\n",
       "0  45.0  13.5000\n",
       "1  34.2   0.0000\n",
       "2  29.0   7.7500\n",
       "3  33.0  15.8500\n",
       "4  48.0   7.8542"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.40</td>\n",
       "      <td>7.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.00</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.00</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.00</td>\n",
       "      <td>59.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75</td>\n",
       "      <td>19.2583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age     Fare\n",
       "0  33.40   7.7750\n",
       "1  34.00   7.7500\n",
       "2  45.00   7.7500\n",
       "3  54.00  59.4000\n",
       "4   0.75  19.2583"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Sample Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works ?** <br>\n",
    "Random sample imputation consists of taking random observation from the dataset and we use this observation to replace the nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should it be used?** <br>\n",
    "It assumes that the data are missing completely at random(MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "1. Easy To implement\n",
    "2. There is less distortion in variance\n",
    "\n",
    "**Disadvantages**\n",
    "1. Every situation randomness wont work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df, features): \n",
    "    for variable in features:\n",
    "        random_sample=df[variable].dropna().sample(df[variable].isnull().sum(), random_state=1)\n",
    "        random_sample.index=df[df[variable].isnull()].index\n",
    "        df.loc[df[variable].isnull(), variable] = random_sample\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Age         0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Age'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Capturing NAN values with a new feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works?**<br>\n",
    "We create new column and put 1 if the value of the feature is missing, otherwise 0. The missing values of the column we replace with mean/median/mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to apply?**<br>\n",
    "It works well if the data are not missing completely at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "1. Easy to implement\n",
    "2. Captures the importance of missing values\n",
    "\n",
    "**Disadvantages**\n",
    "1. Creating Additional Features(Curse of Dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Way\n",
    "# instead of mean we can use median/mode.\n",
    "def impute_nan(df, features):\n",
    "    for feature in features:\n",
    "        df[feature+'_nan'] = np.where(df[feature].isnull(),1,0)\n",
    "        df[feature] = df[feature].fillna(df[feature].mean())\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age     Fare  Age_nan\n",
       "0         0  22.0   7.2500        0\n",
       "1         1  38.0  71.2833        0\n",
       "2         1  26.0   7.9250        0\n",
       "3         1  35.0  53.1000        0\n",
       "4         0  35.0   8.0500        0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Age'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Way\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "\n",
    "\n",
    "def impute_nan(X_train, X_test, strategy='mean', missing_values=np.nan):\n",
    "    \n",
    "    transformer = FeatureUnion( transformer_list=[\n",
    "                                                  ('features', SimpleImputer(missing_values=missing_values, strategy=strategy)),\n",
    "                                                  ('indicators', MissingIndicator(missing_values=missing_values))])\n",
    "    \n",
    "    transform_X_train = transformer.fit_transform(X_train, y_train)    \n",
    "    transform_X_test = transformer.transform(X_test)\n",
    "    \n",
    "    return transformer, transform_X_train, transform_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer, new_X_train, new_X_test = impute_nan(X_train, X_test, strategy='mean', missing_values=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.        , 57.        ,  0.        ],\n",
       "       [21.        ,  8.05      ,  0.        ],\n",
       "       [14.5       , 14.4542    ,  0.        ],\n",
       "       ...,\n",
       "       [19.        , 26.2833    ,  0.        ],\n",
       "       [34.        , 13.        ,  0.        ],\n",
       "       [29.48216015, 52.        ,  1.        ]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26.        ,  16.1       ,   0.        ],\n",
       "       [ 34.        ,  26.        ,   0.        ],\n",
       "       [  9.        ,  31.3875    ,   0.        ],\n",
       "       [ 16.        ,  18.        ,   0.        ],\n",
       "       [ 16.        ,   9.2167    ,   0.        ],\n",
       "       [ 45.5       ,   7.225     ,   0.        ],\n",
       "       [ 37.        ,   9.5875    ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [  4.        ,  29.125     ,   0.        ],\n",
       "       [ 29.48216015,   7.8792    ,   1.        ],\n",
       "       [ 29.48216015,  22.3583    ,   1.        ],\n",
       "       [ 50.        , 133.65      ,   0.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 54.        ,  59.4       ,   0.        ],\n",
       "       [ 32.        ,  15.5       ,   0.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 17.        ,   8.6625    ,   0.        ],\n",
       "       [ 29.        ,   9.5       ,   0.        ],\n",
       "       [ 49.        ,   0.        ,   0.        ],\n",
       "       [ 29.48216015,  89.1042    ,   1.        ],\n",
       "       [ 29.48216015,   7.8792    ,   1.        ],\n",
       "       [ 44.        ,   8.05      ,   0.        ],\n",
       "       [ 17.        ,   7.0542    ,   0.        ],\n",
       "       [ 45.        ,  26.55      ,   0.        ],\n",
       "       [ 29.48216015,  56.4958    ,   1.        ],\n",
       "       [ 41.        ,  20.2125    ,   0.        ],\n",
       "       [  3.        ,  15.9       ,   0.        ],\n",
       "       [ 29.48216015,  35.        ,   1.        ],\n",
       "       [ 20.        ,  15.7417    ,   0.        ],\n",
       "       [ 36.        ,  15.55      ,   0.        ],\n",
       "       [ 29.48216015,   7.8958    ,   1.        ],\n",
       "       [ 27.        ,   7.925     ,   0.        ],\n",
       "       [ 32.        ,  26.        ,   0.        ],\n",
       "       [ 39.        ,  83.1583    ,   0.        ],\n",
       "       [ 12.        ,  11.2417    ,   0.        ],\n",
       "       [ 34.        ,  13.        ,   0.        ],\n",
       "       [ 20.        ,   9.8458    ,   0.        ],\n",
       "       [ 39.        ,   0.        ,   0.        ],\n",
       "       [ 30.        ,  93.5       ,   0.        ],\n",
       "       [ 18.        ,  13.        ,   0.        ],\n",
       "       [  2.        ,  31.275     ,   0.        ],\n",
       "       [ 35.        ,  90.        ,   0.        ],\n",
       "       [ 64.        , 263.        ,   0.        ],\n",
       "       [ 25.        ,   7.775     ,   0.        ],\n",
       "       [ 23.        ,  63.3583    ,   0.        ],\n",
       "       [ 29.48216015,  13.8625    ,   1.        ],\n",
       "       [ 49.        ,  89.1042    ,   0.        ],\n",
       "       [ 23.        ,   9.225     ,   0.        ],\n",
       "       [  7.        ,  26.25      ,   0.        ],\n",
       "       [ 29.48216015,  56.4958    ,   1.        ],\n",
       "       [  2.        ,  27.9       ,   0.        ],\n",
       "       [  5.        ,  31.3875    ,   0.        ],\n",
       "       [ 27.        ,  11.1333    ,   0.        ],\n",
       "       [ 59.        ,   7.25      ,   0.        ],\n",
       "       [ 37.        ,  29.7       ,   0.        ],\n",
       "       [ 50.        ,   8.05      ,   0.        ],\n",
       "       [  3.        ,  21.075     ,   0.        ],\n",
       "       [ 28.5       ,  16.1       ,   0.        ],\n",
       "       [ 30.        ,  12.35      ,   0.        ],\n",
       "       [  9.        ,  20.525     ,   0.        ],\n",
       "       [ 29.48216015,  69.55      ,   1.        ],\n",
       "       [ 22.        ,   7.5208    ,   0.        ],\n",
       "       [ 14.        ,  11.2417    ,   0.        ],\n",
       "       [ 27.        ,  13.        ,   0.        ],\n",
       "       [ 16.        ,   7.7333    ,   0.        ],\n",
       "       [  4.        ,  39.        ,   0.        ],\n",
       "       [ 42.        ,   8.6625    ,   0.        ],\n",
       "       [ 48.        ,  13.        ,   0.        ],\n",
       "       [ 20.        ,   7.8542    ,   0.        ],\n",
       "       [ 38.        , 153.4625    ,   0.        ],\n",
       "       [ 47.        ,   9.        ,   0.        ],\n",
       "       [ 42.        ,   8.4042    ,   0.        ],\n",
       "       [  2.        ,  39.6875    ,   0.        ],\n",
       "       [ 40.        ,  27.7208    ,   0.        ],\n",
       "       [  4.        ,  13.4167    ,   0.        ],\n",
       "       [ 62.        ,  26.55      ,   0.        ],\n",
       "       [ 74.        ,   7.775     ,   0.        ],\n",
       "       [ 15.        ,   7.2292    ,   0.        ],\n",
       "       [ 25.        ,   7.8958    ,   0.        ],\n",
       "       [ 18.        ,  13.        ,   0.        ],\n",
       "       [ 36.        , 120.        ,   0.        ],\n",
       "       [  8.        ,  26.25      ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [ 18.        ,  11.5       ,   0.        ],\n",
       "       [ 39.        ,   7.925     ,   0.        ],\n",
       "       [ 22.        ,   9.8375    ,   0.        ],\n",
       "       [ 19.        ,  10.5       ,   0.        ],\n",
       "       [ 28.        ,   7.8958    ,   0.        ],\n",
       "       [ 17.        ,  14.4583    ,   0.        ],\n",
       "       [ 30.        , 106.425     ,   0.        ],\n",
       "       [ 15.        ,  14.4542    ,   0.        ],\n",
       "       [ 52.        ,  13.5       ,   0.        ],\n",
       "       [ 36.        ,  10.5       ,   0.        ],\n",
       "       [ 35.        , 512.3292    ,   0.        ],\n",
       "       [ 61.        ,  32.3208    ,   0.        ],\n",
       "       [ 35.        ,  21.        ,   0.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 18.        ,   8.3       ,   0.        ],\n",
       "       [ 29.48216015,  14.4583    ,   1.        ],\n",
       "       [ 28.        ,  82.1708    ,   0.        ],\n",
       "       [ 31.        ,   7.925     ,   0.        ],\n",
       "       [  1.        ,  37.0042    ,   0.        ],\n",
       "       [ 30.        ,  21.        ,   0.        ],\n",
       "       [ 27.        ,  21.        ,   0.        ],\n",
       "       [ 36.        ,  13.        ,   0.        ],\n",
       "       [ 45.        ,  26.25      ,   0.        ],\n",
       "       [  1.        ,  39.        ,   0.        ],\n",
       "       [ 40.        ,  39.        ,   0.        ],\n",
       "       [  3.        ,  26.        ,   0.        ],\n",
       "       [ 40.        ,   9.475     ,   0.        ],\n",
       "       [ 33.        ,  15.85      ,   0.        ],\n",
       "       [ 36.        , 512.3292    ,   0.        ],\n",
       "       [ 28.        ,  13.        ,   0.        ],\n",
       "       [ 26.        ,  10.5       ,   0.        ],\n",
       "       [ 29.48216015,   0.        ,   1.        ],\n",
       "       [ 36.        ,   0.        ,   0.        ],\n",
       "       [ 28.        ,  14.4       ,   0.        ],\n",
       "       [ 29.48216015,  15.5       ,   1.        ],\n",
       "       [ 65.        ,  26.55      ,   0.        ],\n",
       "       [ 23.        ,  15.0458    ,   0.        ],\n",
       "       [ 56.        ,  35.5       ,   0.        ],\n",
       "       [ 41.        ,  39.6875    ,   0.        ],\n",
       "       [ 18.        ,   8.05      ,   0.        ],\n",
       "       [ 47.        ,  15.        ,   0.        ],\n",
       "       [ 29.48216015,   0.        ,   1.        ],\n",
       "       [ 29.48216015,   7.8958    ,   1.        ],\n",
       "       [ 26.        ,   7.8958    ,   0.        ],\n",
       "       [ 29.48216015,   7.225     ,   1.        ],\n",
       "       [ 30.        ,  13.        ,   0.        ],\n",
       "       [ 25.        ,  13.        ,   0.        ],\n",
       "       [ 16.        ,   7.775     ,   0.        ],\n",
       "       [ 43.        ,   6.45      ,   0.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 29.48216015,   0.        ,   1.        ],\n",
       "       [ 36.        ,  26.2875    ,   0.        ],\n",
       "       [ 20.5       ,   7.25      ,   0.        ],\n",
       "       [ 34.        ,  21.        ,   0.        ],\n",
       "       [ 51.        ,  61.3792    ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [ 29.48216015,   8.05      ,   1.        ],\n",
       "       [ 43.        , 211.3375    ,   0.        ],\n",
       "       [ 18.        ,   7.775     ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [ 29.48216015,   6.8583    ,   1.        ],\n",
       "       [ 48.        ,   7.8542    ,   0.        ],\n",
       "       [ 50.        ,  10.5       ,   0.        ],\n",
       "       [ 35.        , 512.3292    ,   0.        ],\n",
       "       [ 36.        ,  27.75      ,   0.        ],\n",
       "       [ 16.        ,  57.9792    ,   0.        ],\n",
       "       [ 65.        ,  61.9792    ,   0.        ],\n",
       "       [ 24.        ,  83.1583    ,   0.        ],\n",
       "       [ 55.        ,  16.        ,   0.        ],\n",
       "       [ 36.        ,  13.        ,   0.        ],\n",
       "       [ 52.        ,  93.5       ,   0.        ],\n",
       "       [ 28.        ,  26.        ,   0.        ],\n",
       "       [ 28.        ,  33.        ,   0.        ],\n",
       "       [ 63.        ,  77.9583    ,   0.        ],\n",
       "       [ 65.        ,   7.75      ,   0.        ],\n",
       "       [ 29.48216015,  15.5       ,   1.        ],\n",
       "       [ 29.48216015,  27.7208    ,   1.        ],\n",
       "       [ 29.48216015,   7.8958    ,   1.        ],\n",
       "       [ 40.        ,  13.        ,   0.        ],\n",
       "       [ 20.        ,   9.825     ,   0.        ],\n",
       "       [ 44.        ,  26.        ,   0.        ],\n",
       "       [ 33.        ,  20.525     ,   0.        ],\n",
       "       [ 28.        ,  26.55      ,   0.        ],\n",
       "       [ 18.        ,  73.5       ,   0.        ],\n",
       "       [ 29.48216015,  14.4542    ,   1.        ],\n",
       "       [ 45.        ,  83.475     ,   0.        ],\n",
       "       [ 25.        ,  30.        ,   0.        ],\n",
       "       [ 40.        ,  15.75      ,   0.        ],\n",
       "       [ 38.        ,   7.8958    ,   0.        ],\n",
       "       [ 29.48216015,  14.4583    ,   1.        ],\n",
       "       [  4.        ,  31.275     ,   0.        ],\n",
       "       [ 54.        ,  23.        ,   0.        ],\n",
       "       [ 29.48216015,   7.7292    ,   1.        ],\n",
       "       [ 21.        ,   7.65      ,   0.        ],\n",
       "       [ 19.        ,   7.8792    ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [ 18.        ,  17.8       ,   0.        ],\n",
       "       [ 29.48216015,   7.2292    ,   1.        ],\n",
       "       [ 18.        , 262.375     ,   0.        ],\n",
       "       [ 36.        ,   7.8958    ,   0.        ],\n",
       "       [ 35.        ,  10.5       ,   0.        ],\n",
       "       [  9.        ,  15.9       ,   0.        ],\n",
       "       [ 29.48216015,  39.6       ,   1.        ],\n",
       "       [ 29.48216015,   7.2292    ,   1.        ],\n",
       "       [ 60.        ,  26.55      ,   0.        ],\n",
       "       [ 38.        ,  31.3875    ,   0.        ],\n",
       "       [ 29.        ,  66.6       ,   0.        ],\n",
       "       [ 29.48216015,   7.75      ,   1.        ],\n",
       "       [ 37.        ,  53.1       ,   0.        ],\n",
       "       [ 24.        ,   9.5       ,   0.        ],\n",
       "       [ 24.        ,   7.7958    ,   0.        ],\n",
       "       [ 19.        ,   0.        ,   0.        ],\n",
       "       [ 30.        ,  16.1       ,   0.        ],\n",
       "       [ 29.        ,  10.5       ,   0.        ],\n",
       "       [ 34.        ,   6.4958    ,   0.        ],\n",
       "       [ 25.        ,  13.        ,   0.        ],\n",
       "       [ 29.48216015,   7.8958    ,   1.        ],\n",
       "       [ 27.        ,  76.7292    ,   0.        ],\n",
       "       [  0.75      ,  19.2583    ,   0.        ],\n",
       "       [ 34.        ,  23.        ,   0.        ],\n",
       "       [ 42.        ,  13.        ,   0.        ],\n",
       "       [  9.        ,  27.9       ,   0.        ],\n",
       "       [ 25.        ,   7.05      ,   0.        ],\n",
       "       [ 29.48216015,  35.5       ,   1.        ],\n",
       "       [ 24.        ,  14.5       ,   0.        ],\n",
       "       [ 27.        ,  13.        ,   0.        ],\n",
       "       [ 29.48216015,   7.8292    ,   1.        ],\n",
       "       [ 29.48216015,  21.6792    ,   1.        ],\n",
       "       [  7.        ,  39.6875    ,   0.        ],\n",
       "       [ 18.        ,  11.5       ,   0.        ],\n",
       "       [ 29.48216015,   7.25      ,   1.        ],\n",
       "       [ 51.        ,   7.0542    ,   0.        ],\n",
       "       [ 29.48216015,   7.8792    ,   1.        ],\n",
       "       [  7.        ,  29.125     ,   0.        ],\n",
       "       [ 19.        ,  53.1       ,   0.        ],\n",
       "       [ 39.        ,   7.925     ,   0.        ],\n",
       "       [ 50.        , 247.5208    ,   0.        ],\n",
       "       [ 54.        ,  14.        ,   0.        ],\n",
       "       [ 29.48216015,  22.3583    ,   1.        ]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(transformer_list=[('features', SimpleImputer()),\n",
       "                               ('indicators', MissingIndicator())])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Of course, we cannot use the transformer to make any predictions. We should wrap this in a Pipeline with a classifier (e.g., a DecisionTreeClassifier) to be able to make predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6547085201793722"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = make_pipeline(transformer, DecisionTreeClassifier())\n",
    "clf = clf.fit(X_train, y_train)\n",
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. End of Distribution imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df, features):\n",
    "    for variable in features:\n",
    "        extreme = df[variable].mean() +3*df[variable].std() \n",
    "        df[variable] = df[variable].fillna(extreme)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Age         0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Age'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Arbitrary Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works?**<br>\n",
    "It consists of replacing NAN by an arbitrary value that we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**<br>\n",
    "1. Easy to implement\n",
    "2. Captures the importance of missingess if there is one\n",
    "\n",
    "**Disadvantages** <br>\n",
    "1. Distorts the original distribution of the variable\n",
    "2. If missingess is not important, it may mask the predictive power of the original variable by distorting its distribution\n",
    "3. Hard to decide which value to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Age         177\n",
       "Fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"titanic.csv\", usecols=[\"Age\",\"Fare\",\"Survived\"])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case i choose 0 as arbitrary value. WE can choose 100, 200 etc.\n",
    "def impute_nan(df, features, arb_val):\n",
    "    for variable in features:\n",
    "        df[variable] = df[variable].fillna(arb_val)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Age         0\n",
       "Fare        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Age'], arb_val)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques of handling missing values for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mode Imputation\n",
    "2. Add a new variable to capture nan values\n",
    "3. Replacing nan values with new category 'Missing'\n",
    "4. Dropping rows with missing values\n",
    "5. Dropping columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mode Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "1. Easy To implement\n",
    "2. Fater way to implement\n",
    "**Disadvantages**\n",
    "1. Since we are using the more frequent labels, it may use them in an over respresented way, if there are many nan's\n",
    "2. It distorts the relation of the most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df, features):\n",
    "    for variable in features:\n",
    "        most_frequent_category = df[variable].mode()[0]\n",
    "        df[variable].fillna(most_frequent_category,inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin            0\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Cabin'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adding a variable to capture NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df, features):\n",
    "    for variable in features:\n",
    "        df[variable] = np.where(df[variable].isnull(),1,0)\n",
    "        mode = df[variable].mode()[0]\n",
    "        df[variable] = df[variable].fillna(mode)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin            0\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Cabin'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Replacing Nan values with new category 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('titanic.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df, features):\n",
    "    for variable in features:\n",
    "        df[variable] = np.where(df[variable].isnull(), \"Missing\", df[variable])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin            0\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_nan(df, ['Cabin'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
